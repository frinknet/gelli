#!/usr/bin/env sh
set -e

# USAGE: gelli prompt - Load the model and ask a question
#
# gelli prompt [model] [loras...] [-- prompt] - Run model inference with specified prompt and optional LoRAs
#
#   MODEL defaults to $GELLI_MODEL or ol:qwen2.5-Coder:0.5b-Instruct
#   LORAS defaults to $GELLI_LORAS (space separated)
#
#   Examples:
#
#     gelli prompt ol:qwen3:0.6b -- "What do you know"
#     cat file  |  gelli prompt ol:qwen2.5-Coder:0.5b-Instruct -- "can you do something with this"

TEMP=${GELLI_TEMP:-0.1}
CTX_SIZE=${GELLI_CTX_SIZE:-0}
BATCH_SIZE=${GELLI_BATCH_SIZE:-512}
OUTPUT_SIZE=${GELLI_OUTPUT_SIZE:-999999}

MODEL=""
LORAS=""
FLAGS="$GELLI_LLAMA_FLAGS"
PROMPT="$GELLI_SYSTEM_PROMPT"

while [ $# -gt 0 ]; do
  case $1 in
    --)
      shift

      if [ -z "$PROMPT" ]; then
        PROMPT="$*"
      else
        PROMPT="$PROMPT\n\n$*"
      fi

      break
      ;;
    *)
      if [ -z "$MODEL" ]; then
        MODEL="$1"
      else
        LORAS="$LORAS $1"
      fi
      ;;
  esac

  shift
done

if [ -z "$MODEL" ]; then
  MODEL=${GELLI_MODEL:-$GELLI_DEFAULT}
  LORAS=${GELLI_LORAS:-}
fi

if [ -z "$PROMPT" ]; then
  PROMPT="You are a helpful AI assistant."
fi

MODEL=$(gelli-models download $MODEL)
LORAS=$(gelli-loras download $LORAS)

if [ -n "$LORAS" ]; then
  for f in $LORAS; do FLAGS="$FLAGS --lora /loras/$f.gguf"; done
fi

exec llama-cli --model "/models/${MODEL##*/}.gguf" $FLAGS \
  --batch-size $BATCH_SIZE \
  --ctx-size $CTX_SIZE \
  --predict $OUTPUT_SIZE \
  --prompt "$PROMPT" \
  --temp $TEMP \
  --simple-io \
  --no-warmup \
  --single-turn \
  --no-conversation \
  2>/dev/null #| sed '1d; 2s/^> //; /^> EOF by user$/d'
