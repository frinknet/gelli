#!/usr/bin/env sh
set -e

TEMP=${GELLI_TEMP:-0.1}
CTX_SIZE=${GELLI_CTX_SIZE:-0}
BATCH_SIZE=${GELLI_BATCH_SIZE:-512}
OUTPUT_SIZE=${GELLI_OUTPUT_SIZE:-999999}

MODEL=${1:-${GELLI_MODEL:-$GELLI_DEFAULT}}

shift || true

PORT=${GELLI_PORT:-7771}
LORAS=${*:-${GELLI_LORAS:-}}
FLAGS=""

MODEL=$(gelli-models download $MODEL)
LORAS=$(gelli-loras download $LORAS)

if [ -n "$LORAS" ]; then
  for f in $LORAS; do FLAGS="$FLAGS --lora /lora/$f.gruf"; done
fi

exec llama-server --model "/models/${MODEL##*/}.gguf" $FLAGS \
  --batch-size $BATCH_SIZE \
  --ctx-size $CTX_SIZE \
  --predict $OUTPUT_SIZE \
  --port "$PORT" \
  --temp $TEMP \
  --jinja \
  --no-warmup \
  --flash-attn on \
  --gpu-layers 99
