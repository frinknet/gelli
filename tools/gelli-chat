#!/usr/bin/env sh
set -e

# USAGE: gelli chat - Load the model and start a conversation
#
# gelli ask [model] [loras...] - Run model inference with specified prompt and optional LoRAs
#
#   MODEL defaults to $GELLI_MODEL or ol:qwen2.5-Coder:0.5b-Instruct
#   LORAS defaults to $GELLI_LORAS (space separated)
#
#   Examples:
#
#     gelli chat ol:qwen3:0.6b -- "What do you know"

TEMP=${GELLI_TEMP:-0.1}
CTX_SIZE=${GELLI_CTX_SIZE:-0}
BATCH_SIZE=${GELLI_BATCH_SIZE:-512}
OUTPUT_SIZE=${GELLI_OUTPUT_SIZE:-999999}

MODEL=""
LORAS=""
FLAGS=""
PROMPT=""

while [ $# -gt 0 ]; do
  case $1 in
    --)
      shift

      PROMPT="$*"

      if [ -z "$MODEL" ]; then
        MODEL=${GELLI_MODEL:-$GELLI_DEFAULT}
        LORAS=${GELLI_LORAS:-}
      fi

      break
      ;;
    *)
      if [ -z "$MODEL" ]; then
        MODEL="$1"
      else
        LORAS="$LORAS $1"
      fi
      ;;
  esac

  shift
done

MODEL=$(gelli-model $MODEL)
LORAS=$(gelli-lora $LORAS)

if [ -n "$LORAS" ]; then
  for f in $LORAS; do FLAGS="$FLAGS --lora /loras/$f.gguf"; done
fi

exec llama-cli --model "/models/${MODEL##*/}.gguf" $FLAGS \
  --ctx-size $CTX_SIZE \
  --batch-size $BATCH_SIZE \
  -n $OUTPUT_SIZE \
  --temp $TEMP \
  --color \
  --conversation \
  --no-display-prompt \
  --system-prompt "$PROMPT" 2>/dev/null
