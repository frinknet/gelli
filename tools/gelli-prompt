#!/usr/bin/env sh
set -e

# USAGE: gelli prompt - Load the model and ask a question
#
# gelli prompt [model] [loras...] [-- prompt] - Run model inference with specified prompt and optional LoRAs
#
#   MODEL defaults to $GELLI_MODEL or ol:qwen:0.6b
#   LORAS defaults to $GELLI_LORAS (space separated)
#   set $GELLI_CTX_SIZE or else uses the model default
#   PROMPT can be provided after -- or via piped stdin
#
#   Examples:
#
#     gelli prompt ol:qwen3:0.6b -- "What do you know"
#     cat file  |  gelli prompt ol:qwen3:0.6b -- "can you do something with this"

TEMP=${GELLI_TEMP:-0.1}
CTX_SIZE=${GELLI_CTX_SIZE:-0}
BATCH_SIZE=${GELLI_BATCH_SIZE:-512}
OUTPUT_SIZE=${GELLI_OUTPUT_SIZE:-999999}

FLAGS="$GELLI_LLAMA_FLAGS"
MODEL=""
LORAS=""
PROMPT=""

while [ $# -gt 0 ]; do
  case $1 in
    --)
      shift

      PROMPT="$*"

      if [ -z "$MODEL" ]; then
        MODEL=${GELLI_MODEL:-$GELLI_DEFAULT}
        LORAS=${GELLI_LORAS:-}
      fi

      break
      ;;
    *)
      if [ -z "$MODEL" ]; then
        MODEL="$1"
      else
        LORAS="$LORAS $1"
      fi
      ;;
  esac

  shift
done

MODEL=$(gelli-model $MODEL)
LORAS=$(gelli-lora $LORAS)

if [ -n "$LORAS" ]; then
  for f in $LORAS; do FLAGS="$FLAGS --lora /loras/$f.gguf"; done
fi

exec llama-cli --model "/models/${MODEL##*/}.gguf" $FLAGS \
  --batch-size $BATCH_SIZE \
  --ctx-size $CTX_SIZE \
  -n $OUTPUT_SIZE \
  --system-prompt "$PROMPT" \
  --temp $TEMP \
  --simple-io \
  --single-turn \
  --no-display-prompt \
  2>/dev/null
