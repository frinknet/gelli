#!/usr/bin/env sh
set -e

# USAGE: gelli ask - Load the model and ask a question
#
# gelli ask [model] [loras...] [-- prompt] - Run model inference with specified prompt and optional LoRAs
#
#   MODEL defaults to $GELLI_MODEL or ol:qwen:0.6b
#   LORAS defaults to $GELLI_LORAS (space separated)
#   set $GELLI_CONTEXT or else uses the model default
#   PROMPT can be provided after -- or via piped stdin
#
#   Examples:
#
#     gelli ask ol:qwen3:0.6b -- "What do you know"
#     cat file  |  gelli ask ol:qwen3:0.6b -- "can you do something with this"

CONTEXT=${GELLI_CONTEXT:-0}
BATCH=${GELLI_BATCH:-512}

MODEL=""
LORAS=""
FLAGS=""
PROMPT=""

while [ $# -gt 0 ]; do
  case $1 in
    --)
      shift

      PROMPT="$*"

      if [ -z "$MODEL" ]; then
        MODEL=${GELLI_MODEL:-$GELLI_DEFAULT}
        LORAS=${GELLI_LORAS:-}
      fi

      break
      ;;
    *)
      if [ -z "$MODEL" ]; then
        MODEL="$1"
      else
        LORAS="$LORAS $1"
      fi
      ;;
  esac

  shift
done

if [ ! -t 0 ]; then
  PROMPT="$PROMPT\n\n$(cat)"
fi

MODEL=$(gelli-model $MODEL)
LORAS=$(gelli-lora $LORAS)

if [ -n "$LORAS" ]; then
  for f in $LORAS; do FLAGS="$FLAGS --lora /loras/$f.gguf"; done
fi

#TODO if we have the server running ask the question there assuming it has the right model
exec llama-cli --model "/models/${MODEL##*/}.gguf" $FLAGS --ctx-size $CONTEXT --batch-size $BATCH --prompt "$PROMPT" --temp 0.1 --no-display-prompt --single-turn
